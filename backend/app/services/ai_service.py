"""
AI service for generating video summaries using Google Gemini.
"""
import google.generativeai as genai
import logging
from app.core.config import settings
from app.services.base_ai_service import BaseAIService

logger = logging.getLogger(__name__)


class GeminiService(BaseAIService):
    """Service for generating AI summaries using Google Gemini."""

    def __init__(self, model_name: str = None):
        """Initialize Gemini API with configuration.

        Args:
            model_name: Optional model name to use. If None, uses settings.GEMINI_MODEL
        """
        self.model_name = model_name or settings.GEMINI_MODEL

        if settings.GEMINI_API_KEY and settings.GEMINI_API_KEY != "your_gemini_api_key_here":
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self._is_configured = True
            logger.info(f"Gemini API configured successfully with model: {self.model_name}")
        else:
            self._is_configured = False
            logger.warning("Gemini API key not set! Please configure GEMINI_API_KEY in .env file")

    @property
    def is_configured(self) -> bool:
        """Check if Gemini API is properly configured."""
        return self._is_configured

    def generate_summary_overview(self, transcript: str, custom_prompt: str = None, system_prompt: str = None) -> str:
        """
        Generate a concise 2-3 sentence summary using Gemini.

        Args:
            transcript: Raw transcript text
            custom_prompt: Custom prompt template (optional). Use {transcript} as placeholder.
            system_prompt: System prompt for model behavior (optional, not used in Gemini)

        Returns:
            Concise overview summary (2-3 sentences)
        """
        if not self.is_configured:
            return "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."

        # Limit transcript length for API efficiency
        limited_transcript = transcript[:settings.TRANSCRIPT_LIMIT_OVERVIEW]

        # Use custom prompt if provided, otherwise use default
        if custom_prompt:
            # Check if prompt contains {transcript} placeholder
            if "{transcript}" in custom_prompt:
                # Traditional prompt: replace placeholder with transcript
                prompt = custom_prompt.replace("{transcript}", limited_transcript)
            else:
                # Modular prompt: append transcript to the end
                prompt = f"{custom_prompt}\n{limited_transcript}"
        else:
            prompt = f"""ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒì˜ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ì˜ìƒì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
- í•µì‹¬ ë©”ì‹œì§€ì™€ ì£¼ìš” ì£¼ì œë§Œ í¬í•¨
- êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- 2-3ë¬¸ì¥ìœ¼ë¡œ ì œí•œ

ìŠ¤í¬ë¦½íŠ¸:
{limited_transcript}

ìš”ì•½:"""

        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": settings.GEMINI_TEMPERATURE,
                    "top_p": settings.GEMINI_TOP_P,
                    "max_output_tokens": settings.GEMINI_MAX_TOKENS_OVERVIEW,
                }
            )
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error generating overview: {str(e)}")
            return "AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_summary_detail(self, transcript: str, custom_prompt: str = None, system_prompt: str = None) -> str:
        """
        Generate a detailed markdown summary using Gemini.

        The summary uses markdown format compatible with SummaryPanel parser:
        - ## for H2 headers
        - ### for H3 headers
        - - for bullet points
        - Emojis are allowed

        Args:
            transcript: Raw transcript text
            custom_prompt: Custom prompt template (optional). Use {transcript} as placeholder.
            system_prompt: System prompt for model behavior (optional, not used in Gemini)

        Returns:
            Detailed markdown summary
        """
        if not self.is_configured:
            return "## âš™ï¸ ì„¤ì • í•„ìš”\\n\\nGemini API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ AI ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        # Limit transcript length
        limited_transcript = transcript[:settings.TRANSCRIPT_LIMIT_DETAIL]

        # Use custom prompt if provided, otherwise use default
        if custom_prompt:
            # Check if prompt contains {transcript} placeholder
            if "{transcript}" in custom_prompt:
                # Traditional prompt: replace placeholder with transcript
                prompt = custom_prompt.replace("{transcript}", limited_transcript)
            else:
                # Modular prompt: append transcript to the end
                prompt = f"{custom_prompt}\n{limited_transcript}"
        else:
            prompt = f"""ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒì˜ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ì˜ìƒì˜ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í•œêµ­ì–´ë¡œ ì‘ì„±
2. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš© (##, ###, -, ë“±)
3. ì£¼ìš” ì„¹ì…˜ì„ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ë¶„
4. ê° ì„¹ì…˜ë³„ë¡œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë¶ˆë¦¿ í¬ì¸íŠ¸(-)ë¡œ ì •ë¦¬
5. ì´ëª¨ì§€ ì‚¬ìš© ê°€ëŠ¥ (## ğŸ’¡, ### ğŸ“Š ë“±)
6. 3-5ê°œì˜ ì£¼ìš” ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±

êµ¬ì¡° ì˜ˆì‹œ:
## ğŸ’¡ [ì£¼ìš” ì£¼ì œ 1]
- í•µì‹¬ í¬ì¸íŠ¸ 1
- í•µì‹¬ í¬ì¸íŠ¸ 2

### [ì„¸ë¶€ ì£¼ì œ]
- ìƒì„¸ ì„¤ëª…

ìŠ¤í¬ë¦½íŠ¸:
{limited_transcript}

ìƒì„¸ ìš”ì•½:"""

        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": settings.GEMINI_TEMPERATURE,
                    "top_p": settings.GEMINI_TOP_P,
                    "max_output_tokens": settings.GEMINI_MAX_TOKENS_DETAIL,
                }
            )
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error generating detail: {str(e)}")
            return "## âš ï¸ ì˜¤ë¥˜\\n\\nAI ìƒì„¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def chat(self, context: str, user_message: str, history: list) -> str:
        """
        Chat with video based on transcript context.

        Args:
            context: Context prompt with transcript
            user_message: User's current question
            history: List of previous messages [{"role": "user|assistant", "content": "..."}]

        Returns:
            AI reply text
        """
        if not self.is_configured:
            return "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."

        try:
            model = genai.GenerativeModel(self.model_name)
            
            # Build contents list for Gemini format
            contents = []
            
            # Add context as first user message
            contents.append({
                "role": "user",
                "parts": [{"text": context}]
            })
            
            # Add conversation history
            for msg in history:
                role = "user" if msg["role"] == "user" else "model"
                contents.append({
                    "role": role,
                    "parts": [{"text": msg["content"]}]
                })
            
            # Add current user message
            contents.append({
                "role": "user",
                "parts": [{"text": user_message}]
            })
            
            response = model.generate_content(
                contents,
                generation_config={
                    "temperature": settings.GEMINI_TEMPERATURE,
                    "top_p": settings.GEMINI_TOP_P,
                    "max_output_tokens": 1000,
                }
            )
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}")
            return "ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def translate_segment(self, text: str) -> str:
        """
        Translate a single text segment to Korean using cost-optimized model.

        Args:
            text: Text segment to translate

        Returns:
            Translated text in Korean
        """
        if not self.is_configured:
            return "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

[ë²ˆì—­ ì›ì¹™]
- ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì •í™•íˆ ì „ë‹¬
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ì›ì–´ ë³‘ê¸° (ì˜ˆ: "Machine Learning (ê¸°ê³„í•™ìŠµ)")
- ëŒ€í™”ì²´ëŠ” í•œêµ­ì–´ ëŒ€í™”ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜

[ì›ë¬¸]
{text}

[ë²ˆì—­]"""

        try:
            # Use cost-optimized translation model (Flash)
            model = genai.GenerativeModel(settings.GEMINI_TRANSLATION_MODEL)
            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 1000,
                }
            )
            return response.text.strip()
        except Exception as e:
            logger.error(f"Error in segment translation: {str(e)}")
            return "ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def translate_batch(self, segments: list) -> list:
        """
        Translate multiple segments in batch using cost-optimized model.

        Args:
            segments: List of text segments to translate

        Returns:
            List of translated texts in Korean
        """
        if not self.is_configured:
            return ["Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."] * len(segments)

        # Join segments with separator
        segments_text = "\n---\n".join(segments)

        prompt = f"""ì•„ë˜ ì˜ì–´ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

[ì¤‘ìš” ê·œì¹™]
1. ì›ë¬¸ì„ í¬í•¨í•˜ì§€ ë§ê³ , ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”
2. ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ë²ˆì—­
3. ë²ˆì—­ ê²°ê³¼ë§Œ "---" êµ¬ë¶„ìë¡œ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥
4. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì •í™•íˆ ì „ë‹¬
5. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©
6. ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ì›ì–´ ë³‘ê¸° (ì˜ˆ: "Machine Learning (ê¸°ê³„í•™ìŠµ)")
7. ëŒ€í™”ì²´ëŠ” í•œêµ­ì–´ ëŒ€í™”ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜

[ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ]
ì…ë ¥: "Hello---How are you?---Thank you"
ì¶œë ¥: "ì•ˆë…•í•˜ì„¸ìš”---ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?---ê°ì‚¬í•©ë‹ˆë‹¤"

[ì…ë ¥ í…ìŠ¤íŠ¸]
{segments_text}

[ë²ˆì—­ ì¶œë ¥ (ë²ˆì—­ë¬¸ë§Œ, ì›ë¬¸ í¬í•¨í•˜ì§€ ë§ ê²ƒ)]"""

        try:
            # Use cost-optimized translation model (Flash)
            model = genai.GenerativeModel(settings.GEMINI_TRANSLATION_MODEL)
            response = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 8000,
                }
            )

            # Split result by separator
            translated_text = response.text.strip()
            translations = [t.strip() for t in translated_text.split("---")]

            # Validate count
            if len(translations) != len(segments):
                logger.warning(
                    f"Translation count mismatch: expected {len(segments)}, got {len(translations)}"
                )
                # Pad with originals if too few
                while len(translations) < len(segments):
                    translations.append(segments[len(translations)])
                # Truncate if too many
                translations = translations[:len(segments)]

            return translations
        except Exception as e:
            logger.error(f"Error in batch translation: {str(e)}")
            return ["ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."] * len(segments)
