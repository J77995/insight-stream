"""
AI service for generating video summaries using OpenAI GPT models.
"""
from openai import OpenAI
import logging
from app.core.config import settings
from app.services.base_ai_service import BaseAIService

logger = logging.getLogger(__name__)


class OpenAIService(BaseAIService):
    """Service for generating AI summaries using OpenAI."""

    def __init__(self, model_name: str = None):
        """Initialize OpenAI API with configuration.

        Args:
            model_name: Optional model name to use. If None, uses settings.OPENAI_MODEL
        """
        self.model_name = model_name or settings.OPENAI_MODEL

        if settings.OPENAI_API_KEY and settings.OPENAI_API_KEY != "your_openai_api_key_here":
            self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
            self._is_configured = True
            logger.info(f"OpenAI API configured successfully with model: {self.model_name}")
        else:
            self.client = None
            self._is_configured = False
            logger.warning("OpenAI API key not set! Please configure OPENAI_API_KEY in .env file")

    @property
    def is_configured(self) -> bool:
        """Check if OpenAI API is properly configured."""
        return self._is_configured

    def generate_summary_overview(self, transcript: str, custom_prompt: str = None, system_prompt: str = None) -> str:
        """
        Generate a concise 2-3 sentence summary using OpenAI.

        Args:
            transcript: Raw transcript text
            custom_prompt: Custom prompt template (optional). Use {transcript} as placeholder.
            system_prompt: System prompt for model behavior (optional)

        Returns:
            Concise overview summary (2-3 sentences)
        """
        if not self.is_configured:
            return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."

        # Limit transcript length for API efficiency
        limited_transcript = transcript[:settings.TRANSCRIPT_LIMIT_OVERVIEW]

        # Use custom prompt if provided, otherwise use default
        if custom_prompt:
            # Check if prompt contains {transcript} placeholder
            if "{transcript}" in custom_prompt:
                # Traditional prompt: replace placeholder with transcript
                prompt = custom_prompt.replace("{transcript}", limited_transcript)
            else:
                # Modular prompt: append transcript to the end
                prompt = f"{custom_prompt}\n{limited_transcript}"
        else:
            prompt = f"""ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒì˜ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ì˜ìƒì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
- í•µì‹¬ ë©”ì‹œì§€ì™€ ì£¼ìš” ì£¼ì œë§Œ í¬í•¨
- êµ¬ì²´ì ì´ê³  ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
- 2-3ë¬¸ì¥ìœ¼ë¡œ ì œí•œ

ìŠ¤í¬ë¦½íŠ¸:
{limited_transcript}

ìš”ì•½:"""

        # Use custom system prompt if provided
        sys_prompt = system_prompt or "ë‹¹ì‹ ì€ YouTube ì˜ìƒì˜ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": sys_prompt
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS_OVERVIEW
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error generating overview with OpenAI: {str(e)}")
            return "AI ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_summary_detail(self, transcript: str, custom_prompt: str = None, system_prompt: str = None) -> str:
        """
        Generate a detailed markdown summary using OpenAI.

        The summary uses markdown format compatible with SummaryPanel parser:
        - ## for H2 headers
        - ### for H3 headers
        - - for bullet points
        - Emojis are allowed

        Args:
            transcript: Raw transcript text
            custom_prompt: Custom prompt template (optional). Use {transcript} as placeholder.
            system_prompt: System prompt for model behavior (optional)

        Returns:
            Detailed markdown summary
        """
        if not self.is_configured:
            return "## âš™ï¸ ì„¤ì • í•„ìš”\\n\\nOpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ AI ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        # Limit transcript length
        limited_transcript = transcript[:settings.TRANSCRIPT_LIMIT_DETAIL]

        # Use custom prompt if provided, otherwise use default
        if custom_prompt:
            # Check if prompt contains {transcript} placeholder
            if "{transcript}" in custom_prompt:
                # Traditional prompt: replace placeholder with transcript
                prompt = custom_prompt.replace("{transcript}", limited_transcript)
            else:
                # Modular prompt: append transcript to the end
                prompt = f"{custom_prompt}\n{limited_transcript}"
        else:
            prompt = f"""ë‹¤ìŒì€ ìœ íŠœë¸Œ ì˜ìƒì˜ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

ì´ ì˜ìƒì˜ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í•œêµ­ì–´ë¡œ ì‘ì„±
2. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš© (##, ###, -, ë“±)
3. ì£¼ìš” ì„¹ì…˜ì„ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ë¶„
4. ê° ì„¹ì…˜ë³„ë¡œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë¶ˆë¦¿ í¬ì¸íŠ¸(-)ë¡œ ì •ë¦¬
5. ì´ëª¨ì§€ ì‚¬ìš© ê°€ëŠ¥ (## ğŸ’¡, ### ğŸ“Š ë“±)
6. 3-5ê°œì˜ ì£¼ìš” ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±

êµ¬ì¡° ì˜ˆì‹œ:
## ğŸ’¡ [ì£¼ìš” ì£¼ì œ 1]
- í•µì‹¬ í¬ì¸íŠ¸ 1
- í•µì‹¬ í¬ì¸íŠ¸ 2

### [ì„¸ë¶€ ì£¼ì œ]
- ìƒì„¸ ì„¤ëª…

ìŠ¤í¬ë¦½íŠ¸:
{limited_transcript}

ìƒì„¸ ìš”ì•½:"""

        # Use custom system prompt if provided
        sys_prompt = system_prompt or "ë‹¹ì‹ ì€ YouTube ì˜ìƒì˜ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ìš”ì•½í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„± ë†’ì€ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": sys_prompt
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS_DETAIL
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error generating detail with OpenAI: {str(e)}")
            return "## âš ï¸ ì˜¤ë¥˜\\n\\nAI ìƒì„¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def chat(self, context: str, user_message: str, history: list) -> str:
        """
        Chat with video based on transcript context.

        Args:
            context: Context prompt with transcript
            user_message: User's current question
            history: List of previous messages [{"role": "user|assistant", "content": "..."}]

        Returns:
            AI reply text
        """
        if not self.is_configured:
            return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."

        try:
            # Build messages list
            messages = [{"role": "system", "content": context}]
            
            # Add conversation history
            messages.extend(history)
            
            # Add current user message
            messages.append({"role": "user", "content": user_message})
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=1000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error in chat with OpenAI: {str(e)}")
            return "ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def translate_segment(self, text: str) -> str:
        """
        Translate a single text segment to Korean using cost-optimized model.

        Args:
            text: Text segment to translate

        Returns:
            Translated text in Korean
        """
        if not self.is_configured:
            return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

[ë²ˆì—­ ì›ì¹™]
- ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì •í™•íˆ ì „ë‹¬
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ì›ì–´ ë³‘ê¸° (ì˜ˆ: "Machine Learning (ê¸°ê³„í•™ìŠµ)")
- ëŒ€í™”ì²´ëŠ” í•œêµ­ì–´ ëŒ€í™”ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜

[ì›ë¬¸]
{text}

[ë²ˆì—­]"""

        try:
            # Use cost-optimized translation model (gpt-4o-mini)
            response = self.client.chat.completions.create(
                model=settings.OPENAI_TRANSLATION_MODEL,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Error in segment translation with OpenAI: {str(e)}")
            return "ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def translate_batch(self, segments: list) -> list:
        """
        Translate multiple segments in batch using cost-optimized model.

        Args:
            segments: List of text segments to translate

        Returns:
            List of translated texts in Korean
        """
        if not self.is_configured:
            return ["OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."] * len(segments)

        # Join segments with separator
        segments_text = "\n---\n".join(segments)

        prompt = f"""ì•„ë˜ YouTube ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ì˜ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

[ë²ˆì—­ ì›ì¹™]
- ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ë²ˆì—­
- ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ì •í™•íˆ ì „ë‹¬
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ëŠ” í•„ìš”ì‹œ ì›ì–´ ë³‘ê¸° (ì˜ˆ: "Machine Learning (ê¸°ê³„í•™ìŠµ)")
- ëŒ€í™”ì²´ëŠ” í•œêµ­ì–´ ëŒ€í™”ì²´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜
- ì„¸ê·¸ë¨¼íŠ¸ êµ¬ë¶„ì„ ìœ„í•´ ê° ë²ˆì—­ ê²°ê³¼ë¥¼ "---" êµ¬ë¶„ìë¡œ ë¶„ë¦¬

[ì›ë¬¸ ì„¸ê·¸ë¨¼íŠ¸]
{segments_text}

[ë²ˆì—­ ì„¸ê·¸ë¨¼íŠ¸]"""

        try:
            # Use cost-optimized translation model (gpt-4o-mini)
            response = self.client.chat.completions.create(
                model=settings.OPENAI_TRANSLATION_MODEL,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=8000
            )

            # Split result by separator
            translated_text = response.choices[0].message.content.strip()
            translations = [t.strip() for t in translated_text.split("---")]

            # Validate count
            if len(translations) != len(segments):
                logger.warning(
                    f"Translation count mismatch: expected {len(segments)}, got {len(translations)}"
                )
                # Pad with originals if too few
                while len(translations) < len(segments):
                    translations.append(segments[len(translations)])
                # Truncate if too many
                translations = translations[:len(segments)]

            return translations
        except Exception as e:
            logger.error(f"Error in batch translation with OpenAI: {str(e)}")
            return ["ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."] * len(segments)
